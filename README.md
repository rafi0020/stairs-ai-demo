# Stairs-AI Demo

> **Portfolio-grade, GitHub Pages‚Äìdeployable demo system for Unilever Smart Staircase Safety monitoring**

[![Deploy to GitHub Pages](https://github.com/YOUR_USERNAME/stairs-ai-demo/actions/workflows/deploy.yml/badge.svg)](https://github.com/YOUR_USERNAME/stairs-ai-demo/actions/workflows/deploy.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

A complete **whitebox AI demonstration** that shows every step of the computer vision pipeline‚Äîfrom YOLO detection through MediaPipe pose estimation to state machine debouncing. Built for transparency and explainability.

## üéØ What This Proves

| Capability | Evidence |
|------------|----------|
| **YOLO Detection** | Person bounding boxes with confidence scores |
| **Pose Estimation** | 33-landmark skeleton overlay via MediaPipe |
| **Rail Compliance** | Polygon hit-test for handrail zones |
| **Phone Detection** | Wrist-to-ear distance heuristic |
| **State Machine** | Debounced transitions preventing flicker |
| **Event Logging** | Structured JSONL with evidence frames |

## üèóÔ∏è Architecture

```mermaid
flowchart TD
    subgraph Input
        V[preview.mp4<br/>8s @ 24fps]
    end
    
    subgraph Python Pipeline
        Y[YOLOv8n<br/>Person Detection]
        C[Crop & Resize]
        P[MediaPipe Pose<br/>33 Landmarks]
        R[Rail Hit-Test<br/>Polygon Contains]
        PH[Phone Heuristic<br/>Wrist‚ÜîEar Distance]
        SM[State Machine<br/>Debounce 0.55s]
        E[Event Logger<br/>JSONL + Evidence]
    end
    
    subgraph Output
        FP[frame_packets.jsonl]
        EV[events.jsonl]
        EF[evidence_frames/]
        SUM[summary.json]
    end
    
    subgraph Web UI
        OV[Overview]
        WB[Whitebox]
        TL[Timeline]
        IN[Inspector]
        TU[Tuning Lab]
    end
    
    V --> Y --> C --> P --> R --> PH --> SM --> E
    E --> FP & EV & EF & SUM
    FP & EV & SUM --> OV & WB & TL & IN & TU
```

## üìÅ Repository Structure

```
stairs-ai-demo/
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ preview.mp4           # 8-second demo video (1280√ó720 @ 24fps)
‚îÇ   ‚îî‚îÄ‚îÄ screenshots/          # README images
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.py          # Dataclasses for all pipeline types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ masks.py          # RailMaskManager with polygon hit-test
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pose.py           # MediaPipe wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phone.py          # Wrist-to-ear phone heuristic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state_machine.py  # Debounce logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ overlay.py        # CV2 drawing utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events.py         # Event logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ io.py             # JSON/JSONL utilities
‚îÇ   ‚îú‚îÄ‚îÄ run_demo.py           # Main inference pipeline
‚îÇ   ‚îú‚îÄ‚îÄ extract_storyboard.py # Chapter thumbnail extraction
‚îÇ   ‚îî‚îÄ‚îÄ make_static_site_payload.py
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ demo/             # Generated static payload
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/              # TypeScript types & utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/            # React page components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/       # Reusable UI components
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ rail_masks.json           # Handrail polygon definitions
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- FFmpeg (optional, for video processing)

### One-Command Pipeline

```bash
# Clone and setup
git clone https://github.com/YOUR_USERNAME/stairs-ai-demo.git
cd stairs-ai-demo

# Create Python environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/macOS

# Install dependencies
pip install -r requirements.txt

# Run inference pipeline (processes assets/preview.mp4)
python tools/run_demo.py

# Extract storyboard thumbnails
python tools/extract_storyboard.py

# Convert to static site payload
python tools/make_static_site_payload.py

# Start web UI
cd web
npm install
npm run dev
```

Open http://localhost:3000 to view the demo.

## üñ•Ô∏è Web UI Pages

### Overview (`/`)
Landing page with metrics summary, storyboard chapters, and architecture diagram.

### Whitebox (`/whitebox`)
Step-by-step pipeline visualization:
1. **Raw Frame** ‚Üí 2. **YOLO Detection** ‚Üí 3. **Person Crop** ‚Üí 4. **Pose Estimation** ‚Üí 5. **Rail Hit-Test** ‚Üí 6. **Phone Heuristic** ‚Üí 7. **State Machine** ‚Üí 8. **Final Output**

### Timeline (`/timeline`)
Visual timeline showing compliance states, phone detection events, and chapter markers.

### Inspector (`/inspector`)
Frame-by-frame JSON viewer with expandable sections for metrics, persons, and state machine data.

### Tuning Lab (`/tuning`)
Interactive parameter adjustment:
- **Phone Threshold**: 0.01 ‚Äì 0.15 (default: 0.05)
- **Debounce Duration**: 0.1s ‚Äì 2.0s (default: 0.55s)

Compare baseline vs. tuned results in real-time.

## ‚öôÔ∏è Configuration

### Rail Masks (`rail_masks.json`)

```json
{
  "left_rail": [[50, 200], [150, 200], [150, 600], [50, 600]],
  "right_rail": [[1130, 200], [1230, 200], [1230, 600], [1130, 600]],
  "frame_width": 1280,
  "frame_height": 720
}
```

### Key Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `yolo_conf` | 0.50 | YOLO confidence threshold |
| `phone_threshold` | 0.05 | Normalized wrist-to-ear distance |
| `min_state_duration_sec` | 0.55 | Debounce window for state changes |

## üîß State Machine

```mermaid
stateDiagram-v2
    direction LR
    
    [*] --> Unknown
    Unknown --> NonCompliant: No rail hit
    Unknown --> Compliant: Rail hit detected
    
    NonCompliant --> PendingCompliant: Rail hit detected
    PendingCompliant --> Compliant: Debounce elapsed (0.55s)
    PendingCompliant --> NonCompliant: Rail hit lost
    
    Compliant --> PendingNonCompliant: Rail hit lost
    PendingNonCompliant --> NonCompliant: Debounce elapsed (0.55s)
    PendingNonCompliant --> Compliant: Rail hit detected
    
    state "Phone Detection" as Phone {
        NoPhone --> PendingPhone: Distance < threshold
        PendingPhone --> PhoneDetected: Debounce elapsed
        PhoneDetected --> PendingNoPhone: Distance >= threshold
        PendingNoPhone --> NoPhone: Debounce elapsed
    }
```

## üöÄ Deployment

### GitHub Pages (Recommended)

1. Fork this repository
2. Enable GitHub Pages in Settings ‚Üí Pages ‚Üí Source: GitHub Actions
3. Push to `main` branch
4. Access at `https://YOUR_USERNAME.github.io/stairs-ai-demo/`

The workflow automatically:
- Builds the Vite app
- Deploys to GitHub Pages
- Configures correct base path

### Manual Build

```bash
cd web
npm run build
# Output in web/dist/
```

## üìä Output Files

| File | Description |
|------|-------------|
| `frame_packets.jsonl` | Per-frame analysis data (metrics, persons, state) |
| `events.jsonl` | State transition events with evidence |
| `summary.json` | Aggregate statistics |
| `evidence_frames/` | JPEG snapshots at event boundaries |
| `preview_annotated.mp4` | Video with overlaid detections |

### Event Types

- `non_compliant_start` ‚Äì Person stopped holding rail
- `compliant_start` ‚Äì Person started holding rail
- `phone_detected` ‚Äì Phone-to-ear pose detected
- `phone_ended` ‚Äì Phone lowered

## ‚ö†Ô∏è Known Limitations

> **These are intentionally demonstrated in the demo video:**

| Limitation | Visible In Demo |
|------------|-----------------|
| Partial occlusion affects pose accuracy | Frame 3.2s ‚Äì arm behind body |
| Fast motion causes detection flicker | Frame 1.8s ‚Äì quick movement |
| Single-camera blind spots | N/A in frontal view |
| Debounce delay in state updates | Frame 2.5s ‚Äì 0.55s lag visible |

## üî¨ Algorithm Details

### Phone Heuristic

```python
def detect_phone_usage(pose, threshold=0.05):
    distances = [
        dist(left_wrist, left_ear),
        dist(left_wrist, right_ear),
        dist(right_wrist, left_ear),
        dist(right_wrist, right_ear),
    ]
    min_dist = min(distances)
    return min_dist < threshold
```

Distances are normalized by the shoulder width for scale invariance.

### Rail Hit-Test

Uses ray-casting algorithm for point-in-polygon test:
```python
def point_in_polygon(x, y, polygon):
    # Cast ray to the right, count intersections
    # Odd count = inside, even = outside
```

## üìù License

MIT License ‚Äì see [LICENSE](LICENSE) for details.

## üôè Acknowledgments

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics) ‚Äì Object detection
- [MediaPipe](https://mediapipe.dev/) ‚Äì Pose estimation
- [Vite](https://vitejs.dev/) + [React](https://react.dev/) ‚Äì Web framework
- [Tailwind CSS](https://tailwindcss.com/) ‚Äì Styling
- [Lucide](https://lucide.dev/) ‚Äì Icons

---

**Built with üíô for Unilever Smart Factory Initiative**
